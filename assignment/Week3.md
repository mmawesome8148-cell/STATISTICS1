# 통계학 3주차 정규과제
📌통계학 정규과제는 매주 정해진 분량의 『데이터 분석가가 반드시 알아야 할 모든 것』 을 읽고 학습하는 것입니다. 이번 주는 아래의 Statistics_3rd_TIL에 나열된 분량을 읽고 학습 목표에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

3주차는 2부-데이터 분석 준비하기를 읽고 새롭게 배운 내용을 정리해주시면 됩니다

## **Statistics_3rd_TIL**<br>
### **2부. 데이터 분석 준비하기**
**08. 분석 프로젝트 준비 및 기획** <br>
**09. 분석 환경 세팅하기**

#### Study ScheduleStudy Schedule<br>
주차	공부 범위	완료 여부 <br>
1주차	1부 p.2~46	✅ <br>
2주차	1부 p.47~81	✅ <br>
3주차	2부 p.82~120	✅ <br>
4주차	2부 p.121~167	🍽️ <br>
5주차	2부 p.168~202	🍽️ <br>
6주차	3부 p.203~250	🍽️ <br>
7주차	3부 p.251~299	🍽️<br>

***
# 1️⃣ 개념 정리
## 08. 분석 프로젝트 준비 및 기획
### ✅ 학습 목표 :
* 데이터 분석 프로세스를 설명할 수 있다.
* 비즈니스 문제를 정의할 때 주의할 점을 설명할 수 있다.
* 외부 데이터를 수집하는 방법에 대해 인식한다.

### **1.데이터 분석 프로세스**
- **목적**: 의사결정 과정 최적화, 효과적인 의사결정
- **과정**: 내외부 데이터 수집 및 전처리 > 분석 모형 및 알고리즘 개발 
- **(1) 설계 단계**: 데이터 분석 목적 설정 및 인력 구성
- **(2) 분석 및 모델링 단계**: 데이터 추출, 검토, 가공, 모델링, 모델링 성능 평가
- **(3) 구축 및 활용 단계**: 최종 선정된 분석 모델을 적용한 후 A/B 테스트를 통해 성과 측정

### **1-1. CRISP-DM 방법론**
- **1단계) 비즈니스 이해**
        - 현재 상황 평가
        - 데이터 마이닝 목표 설정
        - 분석 계획 수립
- **2단계) 데이터 이해**
        - 데이터 설명, 탐색, 품질 확인
- **3단계) 데이터 준비**
        - 데이터 선택, 정제
        - 필수 데이터 구성
        - 데이터 통합
- **4단계) 모델링**
        - 모델링 기법 선정
        - 모델 생성
        - 모델 평가
- **5단계) 평가**
        - 결과 평가
        - 프로세스 검토
        - 다음 단계 결정
- **6단계) 배포**
        - 배포 계획
        - 모니터링 및 유지 관리
        - 보고서 작성, 검토

### **1-2. SAS SEMMA 방법론**
- **1단계) SAMPLING**
        - 전체 데이터에서 분석할 것만 추출
        - 데이터 분할 및 병합
        - 표본 추출을 통해 대표성 가진 분석용 데이터 생성
        - 분석 모델 생성위해 학습/ 검증/ 테스트 데이터셋 분할
- **2단계) EXPLORATION**
        - 통계치, 그래프를 통해 데이터 분석
        - 상관변수, 클러스터링을 통해 상관관계 파악
        - 분석 모델에 적합한 변수 설정
        - 데이터 현황 파악 후 비즈니스 아이디어 및 분석 방향 수정
- **3단계) MODIFICATION**
        - 결측값 처리 및 최종 분석 변수 선정
        - 로그 변환, 구간화등 데이터 가공
        - 주성분 분석을 통해 새로운 변수 생성
- **4단계) MODELING**
        - 다양한 데이터마이닝 기법 적용 검토
        - 비즈니스 목적에 맞는 분석 모델 선정 및 알고리즘 적용
        - 데이터 형태에 따라 지도학습, 비지도학습, 강화학습
        - 분석환경 인프라 및 모델 성능 정확도를 고려해 모델 세부 옵션 설정
- **5단계) ASSESSMENT**
        - 구축한 모델들의 예측력, 성능 비교, 분석, 평가
        - 비즈니스 상황에 맞는 적정 임계치 설정
        - 분석 모델 결과응 비즈니스 인사이트에 적용
        - 상황에 따라 추가적인 데이터 분석 수행

### **1-3. 공통 내용**
- **초반부**: 비즈니스 문제와 분석, 해결방향을 명확히 정의하고 데이터를 탐색
- **중반부**: 데이터를 목적에 맞게 수집 및 가공하고 필요에 따라 머신러닝 모델을 사용한다. 
- **후반부**: 데이터 분석 결과를 검토 및 검증하고 실제 환경에 적용한다. 적용 이후 지속적으로 성과를 측정하고 모델을 보완해야한다. 


### **2. 비즈니스 문제 정의와 분석 목적 도출**
- 데이터 분석 프로젝트 시작 전, 현재의 문제 상황을 명확히 정의하고 분석 목적을 분명하게 정해야 함.
- 그렇지 않으면 최종 인사이트 도출 및 솔루션 적용 단계에서 데이터 분석 결과의 효과를 보기 어렵다. 

### **2-1. 비즈니스 문제 정의 방식: MECE**
- 상호배탁적이면서 전체를 포괄한다는 의미.
- 로직 트리를 이용해 나뭇가지가 뻗어나가듯 세부 항목들을 정리하는 방식
- 비즈니스 문제 정의에서 문제의 본질이 전달돼야 한다.(단순 현상 설명X)
- **페이오프 매트릭스**: 우선순위 결정방식, 사업성과 높은 과제 중에서 실행가능성이 쉬운 것부터 어려운 순으로 해결. 나머지 과제는 제외.
![페이오프 매트릭스]](3.png)

### **2-2. 분석 목적의 전환**
- 데이터 탐색 전 숨겨진 인사이트를 파악하기 어렵기 때문에 분석 방향이 언제든 바뀔 수 있다.

### **2-3. 도메인 지식**
- **정의**: 해당 분야의 업에 대한 이해도
- 도메인 지식 없으면 1차원적 분석만 가능.  
- 도메인 지식이 있어야 의미있는 변수를 선택하고 분석 방향을 설정할 수 있음. 
- **도메인 지식 습득 방법**
    - 비즈니스 도메인에 소속된 실무자에게 적극적으로 질문하고 자료 요청하기
    - 논문을 참고하여 도메인에 대한 심도있는 지식 습득하기
    - 데이터가 만들어지는 과정을 직접 보기
- 결국 도메인 지식이 있어야 문제를 도출하고 해결할 수 있는 것. 

### **3. 외부 데이터 수집과 크롤링**
- 기업의 내부 데이터만으로 인사이트 도출에 한계를 느낄 떄 외부 데이터를 수집한다. 
- 분석 목적에 맞는 외부 데이터 수집. 
- **외부데이터 수집 방법**
    - 1) 데이터 판매 업체로부터 구매 or MOU를 통해 데이터 공유
    <br> -> 고품질의 데이터
    - 2) **공공 오픈 데이터**
    <br> -> 비용, 노력 적게 필요하지만, <br>데이터 가공에 시간 소요, 저품질.
    - 3) **크롤링**: 웹 페이지에 있는 모든 데이터 수집
    <br> -> 원하는 데이터를 실시간으로 수집 가능, <br> 웹 리뉴얼되면 프로그래밍 코드 수정, <br> 법적 이슈 발생 가능
     + **스크래핑**: 웹 페이지에서 원하는 부분의 정보만 수집

**3-1. 크롤링 방법**
- OpenAPI 통해 데이터 제공
- 코딩 (예- 파이썬의 BeautifulSoup/ Selenium 라이브러리)

***

## 09. 분석 환경 세팅하기
#### ✅ 학습 목표 :
* 데이터 분석의 전체적인 프로세스를 설명할 수 있다.
* 테이블 조인의 개념과 종류를 이해하고, 각 조인 방식의 차이를 구분하여 설명할 수 있다.
* ERD의 개념과 역할을 이해하고, 기본 구성 요소와 관계 유형을 설명할 수 있다.

### **1. 데이터 분석 언어**

- **SAS**
    - 프로그램 언어라기보다는 솔루션
    - 신뢰도, 정확도가 높아 금융업계에서 많이 쓰임. 
    - 데이터 시각화 쉽게 할 수 있음. 
    - 그러나 비용이 높고 딥러닝, 인공신경망 분석에서 약함. 

- **R** 
    - 오픈소스 데이터 분석용
    - 효과적인 데이터 시각화
    - 커뮤니티 활용으로 질의응답 해결 가능

- **파이썬**
    - 오픈소스
    - 데이터에 국한되지 않는 유연한 언어
    - 시각화 면에서 R에 비해 직관적이지 X

- **SQL** 

### **2. 데이터 처리 프로세스 이해하기**
![데이터 흐름](./images/3.png)

- **1단계) OLTP**: 데이터를 거래 단위로 수집, 분류, 저장하는 시스템
- **(1.5단계) ODS**: DW에 저장하기 전 데이터 임시 보관소 - 목적: 최신 데이터 반영)
- **2단계. DW**: 데이터 창고, 원하는 데이터를 쉽게 추출할 수 있도록 저장해둔 통합 데이터베이스 (목적- 전체 히스토리 데이터 보관)
- **3단계) DM**: 사용자 목적에 맞게 일부 데이터가 저장된 곳 (예- 각 부서/ 집단의 필요에 맞게 가공된 개별 저장소)
- **3.5단계) ETL**
    - **추출 (Extract)**: 원천 데이터에서 필요한 데이터 추출
    - **변환(Transform)**: raw 데이터를 정리, 필터링, 정형화
    - **불러내기(Load)**: 변환된 데이터 새로운 테이블에 적재하기
- **4단계) OLAP**: 저장된 데이터를 분석, 요약하는 시스템

### **2-1. 분산 데이터 처리**
- 한 컴퓨터가 해야할 일을 여러 컴퓨터가 나눠서 한 후, 그 결과를 합치는 것.
- **Scale-up 방식**: '하나의' 컴퓨터 용량을 키우고 더 빠른 프로세서 탑재 -> 데이터 크기가 커지면 처리 속도가 급격히 느려짐. 
- **Scale-out 방식**: '여러 대의' 컴퓨터를 병렬로 연결 -> 연산 속도가 빨라서 효율이 훨씬 높음. 

### **2-2. 분산처리방법(1): HDFS**
- **슬레이브 노드**: 데이터 저장, 계산
- **마스터 노드**: 대량의 데이터를 HDFS에 저장, '맵리듀스' 방식으로 데이터 병렬 처리
- **클라이언트 머신**: 병렬 처리된 결과 사용자에게 보여줌. 

### **2-3. 맵리듀스**: Map단계 + Reduce단계
 - **분할 단계**: 입력된 데이터 고정된 크기로 분할
 - **매핑 단계**: 분할된 데이터를 key-value 형태로 묶고 계산
- **셔플링 단계**: 계산 결과 정렬 및 병합
- **리듀싱 단계**: 최종 결과값 산출

### + **하둡**
    - 전체 클러스터 리소스 관리
    - 수행 중인 잡들의 진행상황, 에러 관리
    - 완료된 잡들의 로그 저장 및 확인


### **2-4. 분산 시스템 구조**
- **노드**: 하나의 컴퓨터
- **랙**: 컴퓨터가 여러대 모인 것
- **클러스터**: 랙들이 모인 것
 ![분산시스템 구조](/5.png)

 - 잡 실행 > 여러 개의 태스트 실행 > 각각의 태스크는 맵과 리듀스를 통해 분산 처리

### **2-5. 아파치 스파크**
- 인메모리 기반의 빠른 데이터 처리가 가능
- 다양한 언어 지원으로 사용 편리함. 


### **3. 테이블 조인**
- **이너조인**: A, B 교집합
- **풀 조인**:  왼 + 교집합+ 오
- **아우터 조인**:
- **레프트/라이트 조인**: 교집합 + 왼/오
- **크로스 조인**: 값이 없더라도 모든 행이 생기도록 데이터 가공을 해야할 때 

### **4. 데이터 단어사전**
: 각 칼럼과 테이블 이름을 정할 때 체계를 약속한 일종의 사전

### **5. 메타 데이터 관리 시스템**
: 데이터가 어디에 저장되어있는지, 어떻게 사용할 것인지 이해할 수 있도롣 데이터에 대한 정보를 관리하는 시스템
- **메타 데이터**: 각 테이블과 그 테이블에 속한 데이터의 속성, 관계를 정의한 데이터

### **6. 테이블 정의서**
: DW, DM등에 적재된 테이블과 칼럼의 한글과 영문명, 데이터 속성, 데이터에 대한 간단한 설명이 정리된 표

### **7. ERD**
: 각 테이블의 구성 정보와 테이블 간 관계를 도식으로 표현한 그림 형태로 데이터 구조를 파악하기 위해 사용된다. 
- **엔티티**: 테이블
- **키 칼럼**: 테이블 연결
    - **기본 키**: 해당 테이블에서 유일하게 구분되는 칼럼-> 중복, 결측값 X
    - **외래 키**: 각 테이블 간에 연결을 만들기 위해 다른 테이블(부모 테이블)에서 참조되는 기본 키
    - **슈퍼 키**: 테이블에서 각 행을 식별할 수 있는 키
    - **후보 키**: 기본키의 조건인 유일성과 최소성을 만족하지만 기본키는 아닌 키
- **식별자**: 연결관계
<br>
***

# 2️⃣ 확인 문제
문제 1.
🧚 아래의 테이블을 조인한 결과를 출력하였습니다. 어떤 조인 방식을 사용했는지 맞춰보세요.

사용한 테이블은 다음과 같습니다.

emp_cd	emp_nm	job	dep_cd
1001	김권택	부장	30
1002	김미정	과장	20
1003	이지민	대리	20
1004	장동혁	사원	10
1005	이승화	사원	30
1006	곽주영	과장	40
1007	조용호	사장	NULL
1008	가나다	대리	40
1009	홍길동	차장	10

dep_cd	dep_nm	location
10	인사팀	서울
20	경리팀	서울
30	영업팀	과천
40	전산팀	대전
50	법무팀	인천
보기: INNER, LEFT, RIGHT 조인

**1-1.**
emp_cd	emp_nm	job	dep_cd	dep_nm	location
1001	김권택	부장	30	영업팀	과천
1002	김미정	과장	20	경리팀	서울
1003	이지민	대리	20	경리팀	서울
1004	장동혁	사원	10	인사팀	서울
1005	이승화	사원	30	영업팀	과천
1006	곽주영	과장	40	전산팀	대전
1007	김태연	사장			
1008	최철원	대리	40	전산팀	대전
1009	노동희	차장	10	인사팀	서울

**LEFT 조인**


**1-2.**
emp_cd	emp_nm	job	dep_cd	dep_nm	location
1001	김권택	부장	30	영업팀	과천
1002	김미정	과장	20	경리팀	서울
1003	이지민	대리	20	경리팀	서울
1004	장동혁	사원	10	인사팀	서울
1005	이승화	사원	30	영업팀	과천
1006	곽주영	과장	40	전산팀	대전
1008	최철원	대리	40	전산팀	대전
1009	노동희	차장	10	인사팀	서울

**INNER 조인**

**1-3.**
emp_cd	emp_nm	job	dep_cd	dep_nm	location
1004	장동혁	사원	10	인사팀	서울
1009	홍길동	차장	10	인사팀	서울
1002	김미정	과장	20	경리팀	서울
1003	이지민	대리	20	경리팀	서울
1001	김권택	부장	30	영업팀	과천
1005	이승화	사원	30	영업팀	과천
1006	곽주영	과장	40	전산팀	대전
1008	최철원	대리	40	전산팀	대전
50	법무팀	인천

**RIGHT 조인**

🎉 수고하셨습니다.